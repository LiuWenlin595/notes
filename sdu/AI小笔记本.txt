深度学习：计算机通过构建较简单的概念来学习复杂概念，绘制出这些概念如何建立在彼此的图上，从而得到较"深"的图，这种方法较深度学习
机器学习：AI系统具备自己获取知识的能力，即从原始数据中提取模式的能力。(需要手工设计)
表示学习：使用机器学习来发掘表示本身，而不仅仅把表示映射到输出
强化学习：一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来学习执行任务
自适应线性单元ADALINE：简单地返回函数f(x)本身地值来预测一个实数，它还可以学习从数据预测这些数
随机梯度下降：用于调节ADALINE权重的训练算法
MNIST:Modified National Institude of Standards and Technology


激励函数：提供规模化的非线性化的能力(激活函数可以引入非线性因素，解决线性模型所不能解决的问题。)
损失函数：单次训练损失、全部训练损失
梯度下降：通过损失函数对W和B进行求导，使得W和B在其相应的正确方向上进行渐进，不断的趋近一个损失函数的极小值。注意同步。(:=)
ΔW = -η(ΔE/ΔW)(E对W的偏导)     ΔV = -η(ΔE/ΔV)(E对V的偏导) 
梯度下降法的问题：学习率难以选取，太大会越过极小值(产生振荡)，太小收敛缓慢；容易陷入局部最优解
网络向量化(向前传播、神经网络预测、正向传播)：就是y=g(WT*x+b)的正向过程
网络梯度下降(反向传播)：通过结果逆向调节每一层的W和B，从而使整个神经网络链条能达到近似最优解
训练过程：正向传播、反向传播、正向传播、反向传播………………不断反复，每次都得到一个新的神经网络，最后得到一个较好的结果
注意事项：正向反向都不能算一层更新一层，要一次性全部更新


CNN：卷积神经网络、输入矩阵、滤波器、特征图、
影响特征图(卷积特征)大小的三个因素：深度(滤波器的个数)，步长，零填充
池化操作几种方式：一般池化(最大化，平均化，加和。作用于图像中不重合的区域)，重叠池化，空金字塔池化(多个scale的pooling，便于处理任意大小的图像)
池化操作一般是针对特征图的。
全连接神经网络：对n-1层和n层而言，n-1层的任意一个节点，都和第n层所有节点有连接。即第n层的每个节点在进行计算的时候，激活函数的输入是n-1层所有节点的加权。


权值收敛条件：误差小于某个预先设定的较小的值；两次迭代之间的权值变化已经很小；设定最大迭代次数，当迭代次数超过最大次数就停止。
单层感知器：激活函数采用sign，只能输出两种可能的值
线性神经网络：激活函数为purelin函数(y=x)
LMS算法：w(n+1)=w(n) + 1/2μ(-ΔJ)
Delta法则：梯度下降
拟合：欠拟合(一般迭代次数不够)，正确拟合，过拟合(数据过于准确以至不符合实际情况，一般表现是测试误差比训练误差大很多)
防止过拟合的方法：增加数据集；正则化方法(C=C0+(λ/2n)*Σw^2，向损失函数加入一项，通过减小w的值使之趋近于0来降低模型复杂度)；Dropout(每次只让隐藏层部分神经元工作，对于权值部分训练)

纠结过和错过的地方：
注意np库提供的操作和python自带操作
np.array([[]])是生成二维数组，np.dot(a,b)是矩阵乘
a =[[]]是生成二维列表，a*b是矩阵相对应数值的乘
X_test = np.array([[1,2],[3,4]])   X_test[1]得到的是一维数组
sigmoid的值域范围是0-1，注意自己的期望Y不要超范围



如何优化算法：
要控制自变量的范围，避免激活函数趋近于饱和，否则梯度(学习信号)趋向于0，计算机学不到东西
sigmoid函数中间部分趋近于直线，非线性的表现不好
sigmoid函数的导数一般小于1，学习信号一层一层传下去会导致学习信号越来越小，传到第四层基本上就没办法再继续学习了
激励函数的导数不能总是大于一，否则可能出现梯度爆炸的问题(学习信号越来越大)
从激活函数的选择入手
从神经网络的层数入手
从神经网路的参数的个数和数值调整




