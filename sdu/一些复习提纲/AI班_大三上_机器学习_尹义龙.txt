第一章	概述
模式识别和机器学习的典型区别：深度学习、强化学习
KNN特点：没有训练的过程；属于非线性分类器；当样本数量很大时计算复杂度高
决策树特点：得到的是一组规则集；决策过程具有可理解性；可以确定哪些属性对于分类最重要；对于矩形的样本分布结果很好
	缺点：上层的错误会影响下层；对于非矩形的样本分布结果不好
支持向量机特点：在解决小样本、非线性问题上有优势；选择合适的核后效果很好
其他学习方法：朴素贝叶斯；神经网络；最小二乘；高斯混合模型；隐马尔可夫模型；动态贝叶斯网络
学习思想：集成学习；深度学习；半监督学习；代价敏感性学习；类别不平衡学习；多标记学习；学示例学习

第二章	重要概念
数据、特征、有标记数据、无标记数据
监督学习、无监督学习、半监督学习、弱监督学习(根据数据有无标记)
训练数据：用于对模型进行训练的数据，以得到针对该模型或解决该问题的合适参数
测试数据：用于验证已训练好的模型对新样本的预测性能
错误率：真实输出和预测结果间的差异
泛化能力：已训练好的模型在全集上的预测性能，而非在训练集上的性能。有标记的全集无法获得，只能用测试集上模型的性能作为模型的泛化能力。
独立同分布(i.i.d)：独立是指每次抽样之间是没有关系的，不会相互影响；同分布是指训练集和全体分布一致、相同；
过拟合：一个模型表现出随机的错误或噪音而不是输入和输出间潜在的关系。过拟合发生于一个模型太过于复杂，比如参数过多。
	解决方法：正则化，早停止
欠拟合：欠拟合发生于一个模型或机器学习算法没有能力得到数据和输出间的潜在关系。
	解决方法：决策树，神经网络
经验误差：测试误差
模型的评价方法：(每种方法都要保证测试集和训练集的分布一致)
	留出法：将数据集划分为训练集和测试集(两份)，对训练集训练，在测试集上测试，得到性能指标
	交叉验证法：将数据集划分为k份，取k-1份进行训练，取1份进行测试，对k份性能指标取平均值
	留一法：交叉验证集的特例，k=样本个数
	自助法：(待看)
模型的性能指标：
	在回归任务中：一般采用输出结果和正确结果的最小二乘进行评估计算
	在分类任务中：错误率、准确率是两个重要的指标
比较测验：我们不能保证模型在测试集上的性能指标等于模型在全集上的性能指标，因此用模型评价的结果进行比较不可行。
	常用的是t检验(待看)
偏差、方差：偏差小数据结果更准、方差小数据结果更稳，泛化误差的结果是偏差和方差的综合

第三章 	特征选择和提取
特征是如何形成的：在特征生成阶段生成，对于先验知识明确的情况下可以人工设计特征，不明确的话需要用特征学习的方式(比如深度学习)获得
多少维的特征是好的：在特征选择阶段完成
如何设计分类器：略
分类器性能评估：略
特征选择是从特征中选择与任务最相关的特征。为了避免冗余和减小计算的开销，我们要从原始特征中选择一部分特征
常见的特征选择的三种策略：
	过滤式：最传统的根据特征评估进行特征选择，然后再训练
	包裹式：将特征置于模型中训练，将错误率作为特征评估，选出特征后再正式训练
	嵌入式：将特征选择和评估都嵌入到训练学习的特征中，特征选择是在训练的过程中完成的
L1范式比L2范式更容易获得稀疏解
特征提取是对选择出的特征做线性或非线性的变换成为新的特征，将原始特征变换到新的空间。有时仅仅观察原始特征无法得到良好的结果，因此需要对特征进行相应的转换。
特征提取方法：线性PCA、LDA；非线性KPCA
PCA(待看)：使得特征变化后的样本具有最大可分性
	优点：保证新空间特征不相关的情况下使得特征维数更少，从而实现降维和特征提取
	缺点：被忽略掉的成分可能包含一些相对独立的信息
LDA(待看)：使得映射之后的样本同类之间更紧致，不同类之间更分离。
	优点：有监督的，有类别知道的，处理分类任务更有优势
	缺点：LDA要求数据满足高斯分布；降低的维度有阈值；会丢失一些信息；可能会出现过拟合
KPCA(待看)：数据在原始空间中无法线性可分，此时可采用核PCA进行特征提取

第四章	贝叶斯决策
基于样本的两步贝叶斯决策：先计算类条件概率密度和先验概率，再代入贝叶斯决策规则中求得决策结果
常见的决策规则：
	最小错误率：计算后验概率，取后验概率最大的结果，最小错误率的贝叶斯决策是使用0-1损失函数的最小风险贝叶斯决策的特例
	最小风险：在计算概率的时候考虑风险，加入决策表。对α个决策进行评估，取最小值
		R(αi|x) = ∑(j) λ(αi,wj)*P(wj|x)
	在限定一类错误率条件下使另一类错误率最小的两类别决策：使用拉格朗日乘子法来建立模型
	最小最大决策：决策者持悲观态度，在先验概率变化的情况下寻找贝叶斯风险最大的决策域，并用此决策域对应的先验概率来进行贝叶斯决策
如何根据样本值估计类条件概率密度和先验概率：
	监督参数估计：知道样本的类别，知道类概率密度函数形式，求函数的参数
	非监督参数估计：不知道样本的类别，知道类概率密度函数形式，求函数的参数
	非参数估计：知道样本的类别，直接求类概率密度函数
参数估计：最大似然估计，贝叶斯估计
非参数估计：P(x|w) = k/n/V  在k/n一定的情况下，V越大说明越稀疏，值越小
	Parzen窗：保证V不变，求k
	KNN邻近：保证k不变，求V；预先确定N的某个函数Kn，然后在x点周围不断增长直至捕获Kn个样本为止

第五章	决策树
每次选择最优属性作为一个节点，决策树最重要的就是属性选择算法
停止条件：当前节点的所有样本值相同；无子节点或者子节点样本值相同；无样本
ID3决策树：根据信息增益来选择属性，信息增益最大的作为最优节点；
	信息增益通过熵值来计算；描述的是选择该属性后熵值减小了多少，Gain = Entropy(D) - ∑i |Di|/|D| * Entropy(Di)
	熵值表现的是集合的纯度，Entropy= - ∑(k) pk*log(pk)，pk表示的是结果每个类别权重所占的比例；纯度越大熵值越小
	缺点：ID3决策树倾向于把选择比较多的属性作为最优属性
C4.5决策树：在ID3决策树的基础上Gain = Gain/IV(a) ；IV(a)是一个与属性个数有关的函数，用于抵消ID3倾向于选择属性分类多的这个特点
	利用二分法来处理连续属性，设定阈值，并可以在决策树上多次出现
决策树容易过拟合，因为训练集往往不是全集的无偏采样，会把其中的噪声学进决策树里，因此需要预剪枝和后剪枝
预剪枝：在达到一定条件后停止在决策树上添加属性；计算添加属性后比不添加属性之前泛化能力(准确度)是否提升，如果提升了则添加
预剪枝克服了过拟合，但是可能会导致欠拟合，因此该方法每次只考虑了一个属性
后剪枝：在决策树构建完成后剪掉一些属性
后剪枝的学习能力一般强于预剪枝，因为他对训练集的学习更充分，相对的计算复杂性也大
样本属性有缺失的情况：属性值选择的问题；缺失值样本划分的问题
	解决属性值选择的问题：Gain(D,a) = μ*Gain(D~,a)   D~是指D中有属性a的样本的子集，μ是D~和D的比值
	解决缺失值样本划分的问题：将缺失值样本置入该属性所有子节点中，并更改其权重为r*w，
			r为该类别有值的样本权重占该属性所有样本权重的比例
决策树的分类是轴平行的

第六章	支持向量机
支持向量机思想：在保证分类正确的情况下，选择使得两个样本间隔最大的分类器，这样可以保证泛化能力最强
margin：分类器到两个样本间的最短距离，位于margin边上的向量叫做支持向量
r = max 2*c/||w||   ，用a=1/c进行归一化 ， 为了简化计算转换为min ||w||^2/2
因为带有yi*(w^T*x+b)>=1的约束，因此使用拉格朗日乘子法，min(w,b) max(α) ||w||^2/2 + ∑ α*(1-yi*(w^T*xi+b))
引入对偶问题的原因：更容易求解，在解决非线性问题中可以更容易地引入核函数
对偶问题：max(α) min(w,b) ||w||^2/2 + ∑ α*(1-yi*(w^T*xi+b))
在KKT条件下，一个问题的解等于其对偶问题的解
	W=Σαi*yi*xi ； ∑αi*yi=0 ； αi>=0 ； yi*(w^T*xi+b)-1>=0 ； α*(yi*(w^T*xi+b)-1)=0
训练过程中对应拉格朗日乘子α非零的点是支持向量
SVM计算过程：通过原问题的对偶问题求对W和b的偏导得到W的解(W=Σαi*yi*xi)和b的约束条件(∑αi*yi=0)，再把得到的式子和约束带入对偶问题求得		α，最后根据求得的α带入W=Σαi*yi*xi求出W，再根据yi、xi、w求出b
SMO算法(序列最小优化算法)：坐标上升法对于带有约束的问题不是普遍适用，SMO算法基于坐标上升法，每次会改变在不破坏约束前提下最小数量的ai，每次的ai的选取可以采用启发式规则(经验法则)，关键步骤使如何在保证满足约束的情况下同步更改几个ai使W得到优化
	Ng讲解了针对两个ai的情况，首先利用约束条件把a1用a2表示出来，然后以a2作为自变量对W求最优解，这样就得到了一组a1a2，再看看是否符合其他约束来进行调整
kernel：当前维度的样本线性不可分，我们用kernel将样本映射到高维空间，然后在高维空间中应用SVM，并且避免了在高维空间中对向量内积的计算
常用核函数：线性核XiXj ； 多项式核 (1+XiXj)^p ； 高斯核 ； sigmoid核
norm软间隔SVM：允许一部分样本分错
SVM解决回归问题

第七章	回归学习
回归：数据的标记是连续的值，属于监督学习
基本线性回归：输入和输出间的关系是线性关系，采用最小二乘作为损失函数，容易过拟合
	E = (y-wX)^T * (y-wX) 
岭回归和罗素回归可以防止模型过拟合，也可以解决样本的特征数远超于样本数的问题
L0范式：向量中非零元素的个数 ； L1范式：绝对值求和 ； L2范式：平方和开根号 ； 无穷范式：向量中最大值
岭回归：加入L2范式，E = (y-wX)^T * (y-wX)+λ||w||_2 ， 通过求导可以得到一个包含I的解析解，满秩从而保证了唯一解
罗素回归：加入L1范式，E = (y-wX)^T * (y-wX)+λ||w||_1 ，使用近端梯度下降法来求解L1范式的解(在0处不可导)
在L1范式约束下，参数w可以被约束成0(用是否戴眼镜来区分男女)；在L2范式的约束下，w每个元素可以接近0但不为0


第九章	神经网络
单层感知机：无隐层；多层感知机：有隐层
BP算法：多层感知机的参数学习方法，并且提出了sigmoid函数用于处理非线性问题
神经网络→深度学习：以前的神经网络隐藏层很少，之后随着计算机计算能力变强，隐藏层越来越多，真正成为深度学习
batch：一次训练一批数据，根据这一批数据共同决定损失和参数调整的方向
一轮训练：把所有batch的数据都训练一遍
早停止的条件：参数不再改变
随机梯度下降、batch梯度下降、mini-batch梯度下降
学习率设置太大不容易收敛，学习率设置太小收敛太慢
多个输出的BP算法：计算前置节点的偏导时要把多个y的偏导都算进去求和
梯度下降方法一般找到的是极小值而不是最小值，解决方法：初始化参数、模拟退火算法、随机梯度下降
softmax分类：e^y(k)/∑i e^yi，通过e的指数计算使得映射到0-1后类别间差距更大
	采用one-hot编码：只有一个输出为1，其他都为0
卷积神经网络：多层感知机对于计算量要求过大，CNN可以有效减少参数
	核心特点：局部感受野(每个节点只与图像中一块区域相连)；滤波器参数共享；池化


题目：试简述您对半监督学习方法Co-training的理解
Co-training是协同训练，是基于分歧的方法使用多学习器的重要代表。
协同训练利用了多视图的“相容互补性”。假设数据拥有两个充分且条件独立视图，“充分”是指每个视图都包含足以产生最优学习器的信息，“条件独立”则是指在给定类别标记条件下两个视图独立。在此情形下，可用一个简单的办法来利用未标记数据：首先在每个视图上基于有标记样本分别训练出一个分类器，然后让每个分类器分别去挑选自己”最有把握的”的未标记样本赋予伪标记，并将伪标记样本提供给另一个分类器作为新增的有标记样本用于训练更新。。。。。。这个“互相学习，共同进步”的过程不断迭代进行，直到两个分类器都不再发生变化，或达到预先设定的迭代轮数为止。

题目：试就您的理解，谈谈您对集成学习的理解并以Bagging为例阐述其实现思路。
集成学习是通过构造并结合多个学习器来完成学习任务，集成中可以只包含同种类型的个体学习器，那么这叫同质集成。同质集成中的个体学习器称为“基学习器”。集成也可以包含不同类型的个体学习器，这样的集成是“异质”的。再称基学习器，而是组件学习器、或直接个体学习器。
集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。
Bagging就是并行式集成学习方法最著名的代表。思想是：给定包含m个样本的数据集，我们有放回的随机取样m次得到m个样本的采样集。初始训练集中有的样本在采样集中多次出现，有的可能从未出现。照这样，我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。在对预测输出进行结合时，通常对分类任务使用简单的投票法，对回归任务使用简单的平均法。这就是Bagging的实现思路。






