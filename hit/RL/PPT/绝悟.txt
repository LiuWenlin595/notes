和dota2主要区别体现：实现了完整的MOBA游戏agent

控制依赖解耦：很难观察不同类型动作(button 和 offset)之间的关系，所以拆开。
	可以简化策略网络的设计，之前的话如果两个不合理的动作被一起选了，就需要做预处理解决，现在可以在选择动作之后做后处理；可以增加行为多样性从而增加探索。
duel clip PPO(离轨自适应)：在大规模离轨数据下，只使用clip的PPO可能会出现ratio*A <<0的情况，所以又加了层max(PPO_loss, c * A)，限制A<0时候的下界
多头值估计：把reward分组，区分不同state-action的作用，只在值网络部分使用，参考alpha star
课程式自学习：扩展英雄池的需要，如果使用无序的agent阵容数据进行训练会导致性能下降
蒙特卡洛树搜索：两个网络，取消了搜索
action mask：限制不必要的探索
attention key：选择目标的时候应该选哪个unit，有利于选择的更准确


解决大英雄池的办法：蒙特卡洛树搜索，课程蒸馏学习



介绍了两个系统
一个是纯RL  19年做的
一个是IL  17年做的

大局观
global intent：隔n帧观察agent，把小地图画格子标记index，如果agent的格子发生变化并且采取了攻击，就是意图改变了
local intent：
微操

data proprecessing
场景切分

对于新英雄，选择10来个特征明显的英雄pre_train model
FPS不同的游戏差别不大，可以尝试迁移

启元世界
星际争霸
智能机器人

字节跳动
用RL代替游戏的数值策划
心流：(玩家沉浸式体验)
reward设计：如果怪物出现概率 > 0.8，reward=-10；如果全局没有怪物出现，reward=-10
玩家很弱小出现大怪物 reward=-8；玩家很强大出现小怪物，reward=0；玩家很弱小出现小怪物和玩家很强出现大怪物，reward=1
惊喜感：根据怪物计算熵，reward=-10；根据泊松分布掉农作物，reward=3
决策动作：{不出怪，出小鸟，出野猪，出大boss}