OpenAI-Five
状态：五个agent的obs几乎相同，不采用图像输入(两个原因)，所有观察对自己做一个归一化(每个时刻动态更新均值和标准差)并裁剪到-5~5
动作：一个主要动作+3个参数(delay, unit target, offset)，如果参数无效的话就在更新的时候mask掉这个参数的值
奖励：零和奖励，随时间加权，团队激励系数
模型：三大块，前半部分对obs预处理是感知，LSTM是主要做决策的brain，后半部分是对LSTM输出的解码
算法：PPO+GAE【 14~17】
梯度计算：NCCL【19】
框架：四大块，rollout worker采集数据，Optimizer计算梯度，forward pass帮助worker交互，controller中央控制
数据采样：异步，rollout每得到256个step都上传到replay buffer里，Optimizer直接从replay buffer里取数据 (这样依旧存在256*16个step的老化)
时间基线：6-12min

评估指标：
TrueSkill(评估性能)，每提升8.3个点相对应的表示和之前版本battle的胜率为80%
SpeedUp(评估速度) = baseline第一次达到某个TrueSkill / trainer第一次达到某个TrueSkill， 分母越小/指标越大表示效果越好

Surgery，Rerun：一个省时间，一个效果好一丢丢
1. 网络隐藏层节点维度增加：
W1和B1需要随机化是为了避免新维度参数的对称性，W2初始化为0是因为添加隐层节点后计算出来的z'和z相同(但是这样也就不可避免地留下了参数对称性的隐患)
2. 输入维度增加：
在obs前面新增了一层编码器，编码器输出obs处理后的隐层特征。把obs输入到编码器里，使用[W 0]就可以保证每次输出的隐层特征的维度是一样的
3. 添加动作或改变环境：
当模型可以输出一个新动作的时候，如果不加干预，已经在旧版本训练好的模型放在新版本上可能效果还没有脚本好，这个时候需要对整个模型重新训练，浪费了大量的计算资源
采用模拟退火从0到1地将这个新动作逐渐加入到模型中(没有说具体怎么加)，当发现TrueSkill效果下降的时候，降低退火率	
4. 删除维度： 直接把节点置为常量

Optimizer中存储的之前版本的梯度的处理：
在训练的前几个小时，把学习率置为0，先把这些梯度消耗掉，然后再开始用新版本的episode的梯度进行更新
对手池中旧参数的处理：
所有的参数模型都需要根据新模型的改变 同步地进行1234步，否则对手的策略就失效了

超参数调优：
学习率、团队奖励系数、策略熵系数、GAE时间视界

英雄选择：
Minimax + 动态规划，构建完整的搜索树，因为复杂度所以没办法支持100个英雄

self-play【18】：80%20%，20%和过去PK是为了避免过拟合导致策略崩溃
每10次迭代把当前版本的参数加入到参数池中并使其评估得分置为池中最高分，每次self-play通过softmax选择对手
如果对手赢了，当前参数的评估得分不变；如果输了，会进行一定的衰减

消融试验：
Batchsize：越大训练越快，但是是非线性的，即提高2倍的batch并不能减少一半的时间
样本老化：采样数据和Optimizer差8个版本基本上就训不起来了，最终是半分钟发一波数据，一分钟训练一次，来使得staleness保持在0~1(老化过于严重on-policy算法就不适用了)
样本重用：定义 重用指标 = Optimizer消耗样本的速度 / rollout worker产生样本的速度，这个指标越低效果越好(重用率高了表示重复采样，会影响样本的多样性)

探索：
策略熵，团队激励系数
随机化环境：初始化状态，车道分配，肉山血量，英雄阵容，出装选择


一些细节：
为了辅助理解agent的行为，又单独定义了网络去学习每一时刻的win probability、net worth rank(决定哪个小笨蛋需要买团队共享道具)、team objectives/enemy buildings
把传送和放置守卫的动作单独拎出来训练
使用脚本控制动作：加点，买装备，交换物品，控制小鸡
cross-hero pool：输入进LSTM的时候取了5个players前25%的向量做了maxpool，算是一个体现agent之间配合的点
observation processing的时候做了个unit  embedding，在后面选择unit target的时候会用到
决策动作延迟5-8帧
使用80个英雄只会在前期训练多花20%的时间，但是为什么不做，不得而知
	