Q1：TRPO中采用MM算法优化的时候为什么很难找到KL散度的最大值，而要用平均值来代替？
A1：可能是因为如果状态空间是连续的话，D KL (p(a|s), q(a|s))没办法遍历所有的状态s，因此采用平均来代替，因为平均可以用积分
Q2：涉及多项优化的知识不会，TRPO中为什么可以把KL散度的优化目标项放在约束里？

Q3：TRPO算法里使用平均KL散度代替最大KL散度，这样放宽条件不会导致MM算法的优化前提条件不成立吗？

Q4：TD方法为什么都用Q值而不用V值？
A4：用V值的话没法评价动作的好坏，没法做出策略

Q5：target网络为什么要软替换参数？
A5：target网络存在的意义：如果没有target网络，损失函数会被定义为r+gamma Q' - Q，我们想要更新Q的时候Q'也会跟着变化，就好像永远抓不住牛
       所以我们搞了个target让牛固定住，以便我们网络的收敛。硬替换就是每次直接重新放置牛的位置，软替换就是每次让牛移动一点点，两种方法各有利弊吧。

Q6：A3C方法的Critic输入到底是s还是s a？

Q7：A3C的熵惩罚是如何定义的？
A7：max信息熵，保证策略的探索性

Q8：A3C有的worker工作时间很长怎么办？(A3C如何解决梯度并行计算导致梯度失效)
A8：(错误理解)首先，worker工作时间长，轨迹长并不会导致更新滞后，即使轨迹策略很差，但是回报只与环境有关，这条轨迹只会告诉agent这个策略不好，而不会影响策略学习
       	A3C采用的是同步更新，即所有worker计算完了以后再一起参与master的更新，所以某一个worker工作时间长的话，其他worker就会idle，计算机利用率低
	一种简单的想法就是异步更新，但是异步更新又会带来梯度失效的问题
	梯度失效是指worker1计算了梯度传给master，master把更新后的梯度赋值给worker1，然后worker2也计算了梯度传给master，master再把更新后的梯度赋值给worker2
	这样就会导致的问题是worker1和worker2里面的梯度不一样，进而采样策略不一样，进而计算出来的新梯度不一样。
	夸张一些，worker1计算梯度需要10min，结果master通过其他worker在这10min内已经更新了100次，
	那么对于on-policy算法来讲，worker1计算的梯度只适用于自己的policy，已经不能再帮助master学习了，甚至对于master还会产生负影响
       	其他的解决办法：时间截断；impala思想：worker只负责提供采样，每采样一个状态动作对都传给master，真正的梯度更新由master来完成	

Q9：优先级DQN如何处理噪声？
A9：首先，如果环境没有出现大的问题，一般不会出现引起r发生很大改变的噪声。
       其次，每次从replay buffer中提取batchsize个数据，噪声的影响并不大。
       再其次，可以设定TD error的阈值以去除噪声，设计标记位来防止噪声多次作用，选择合适的采样方法来降低噪声被选中的概率。
       再再其次，可以根据经验对在replay buffer中存留的时间，定期清除存留时间长的数据。

Q10：重要性采样Π=0导致整个轨迹的采样比为0怎么办？
A10：带控制变量的每次决策型方法，G t:h = ratio(R t+1 + γG t+1:h) + (1-ratio)*V(St)，后面那一项称为控制变量，这样保证了即使ratio为0也依然有值

Q11：不打破数据相关性，神经网络的训练效果为什么就不好？(replay buffer的作用之一，另一个作用是使得数据可以重复利用、利用率高)
A11：在神经网络中通常使用随机梯度下降法。随机的意思是我们随机选择一些样本来增量式的估计梯度，比如常用的采用batch训练。如果样本是相关的，那就意味着前后两个batch的很可能也是相关的，那么估计的梯度也会呈现出某种相关性。如果不幸的情况下，后面的梯度估计可能会抵消掉前面的梯度量。从而使得训练难以收敛。
#  如果数据相关性大的话，数据会朝一个方向更新很大，这会导致训练中出现方差大的问题，不宜收敛

Q12：为什么on-policy算法可以收敛？

Q13：为什么A3C可以用n-step而A2C用one-step？

Q14：action_mask在forward前面做和forward后面做有什么区别？
A14：在前面做会增加探索，在后面做效果不理想。假设actor现在等概率输出10个备选动作，但action_mask只能选3个动作。如果在后面做的话，所有不可选的动作都会被重定义为action1，即P(action1)=8/10，P(action2)=P(action3)=1/10
但是在前面做的话，网络只会对三个可选的动作进行softmax，即P(action1)=P(action2)=P(action3)=1/3。action1一般是默认动作，所以在前面做action_mask可以保持更多的探索，而不是clip到默认动作

Q15：深度强化学习网络层数为什么普遍很浅(不超过8层)？
A15：1.由于数据效率低下又缺乏直接监督信号，并不擅长以end-to-end的方式训练过深的网络  
         2.强化学习有快速拟合的要求(网络会在 训练完全结束前 被使用，且训练数据的分布不固定)

Q16：为什么DRL训练效率低下？
A16：强化学习的优化目标是折扣累加的长期收益，使得reward起作用的方式较为间接，并不像有监督学习直接可以用标签来计算loss

Q17：reward rescale到底是怎么做的？
A17：网上有两种理解，一种是只除标准差不减均值，另一种是标准化。做reward rescale的作用就是因为V(s) = Σ γ*r，所以可以减小V的方差
	如果减均值的话，就可能出现一个问题，人工定义的正奖励结果变成了-reward，相对应的V(s)也就变成负的了，网络更新的方向就错了
	但是现在一般都是用A来计算的，用A的话感觉减不减均值差别不大，起到减小方差的作用才是最关键的

Q18：为什么A可以减小方差？
A18：A = G - V，如果V估计准确的话，实际上A就是对G做了一个居中处理，所以一幕数据所有的Gt都会在0的位置附近。举个例子，在状态s可以等概率做三个动作，一个好动作G=+100和两个坏动作G=-100，
	所以算出来的A就是+66和-33，明显可以看到不管是奖励还是惩罚都要更靠近0一些。很多的这样靠近0的A集中在一起求方差，自然就会比G算出来的方差要小。

Q19：为什么data2的self-play要设置成80 20？
A19：个人理解，如果80设高了容易过拟合，如果20设高了策略的优化速度会变慢，但是会更鲁棒

Q20：探索的epsilon固定不变的时候，DQN什么情况下学不到东西，举例子说明？
A20：稀疏奖励的情况下

Q21：你说actor的更新是受到critic的指导的，那具体是如何进行指导的？
A21：主要是考虑PG和AC的区别，critic一方面被用在了target value，极大地减小了方差；另一方面还可以用在baseline，也是减小方差提高训练效果

Q22：如果我的Replay Buffer中现在只有128个steps，但一直到第128个steps都没有遇到terminal state，此时我应该如何计算GAE？（用伪状态值代替）
A22：

Q23：时间惩罚到底有没有意义？
A23：分情况讨论吧
假设我现在的任务20步就可以完成。然后我有两个episode，一个走了20步，一个走了100步，那么20步的reward折扣完了之后肯定比100步的大。
如果我加上时间惩罚的话，20步第一个action的Q=gamma^20 * reward + 20个时间惩罚的等比求和，而100步第一个action的Q=gamma^100 * reward + 100个时间惩罚的等比求和
这样看来时间惩罚的作用和折扣奖励，实际上表达的是一个意思，所以属于鸡肋。这是gamma可以传到100步的情况。

如果gamma传不到100步的话， 100步第一个action的Q就接收不到100步后的奖励，就只有时间惩罚了，这样时间惩罚就有了存在的价值
而当gamma=1的时候，就不存在奖励折扣的说法，这时候时间惩罚就非常有用了
看dota2论文里奖励设计没有用到时间惩罚，绝悟里的奖励设计定义了非常非常非常小的时间惩罚，所以说可以把时间惩罚当作一个鸡肋来看待

Q24：可不可以用10个DQN训练10个agent完成MARL任务？
A24：1.很浪费资源，没必要，应该根据任务的类型细分。   
         2.可以训练，但是训练不出很好的效果。每个agent的输入只能观察到其他agent的状态，但是不知道其他agent的动作，(好像就算是MARL也会存在这个问题) 

Q25：A3C，IMPALA，SEED RL的区别
A25：A3C的actor负责和环境交互，前向传播生成动作，反向传播计算梯度，梯度回传到learner，从learner获得新的模型参数
	IMPALA的actor负责和环境交互，前向传播生成动作，整理trajectory传给learner(反向传播由learner负责)，从learner获得新的模型参数
	SEED RL的actor负责和环境交互，把observation传给learner，由learner负责生成动作和更新梯度(前向传播和反向传播都由learner负责)，actor没有单独的model

Q26：基于熵的RL的优点？
A26：1.exploration：探索更多的策略，当环境突然发生微变导致某一轨迹失效时，可以通过其他策略达到目标
	2.pretrain：基于熵训练的agent，在其它任务下进行微调，可以表现出更强的适应性。


经典问题：

Q1：增强探索，解决EE问题？
A1：DQN、DDPG的ε-greedy(off-policy)，策略梯度算法的概率分布输出，A3C的多线程和策略熵，DDPG的action添加噪声以及policy网络参数后期添加噪声，
       绝悟的控制依赖解耦，随机化环境，编写人工脚本，模仿学习，好奇心机制，未完待续
      TODO(Liuwenlin) count-based exploration、parameter noise

Q2：onpolicy和offpolicy？
A2：1. 是否打破数据相关性  2. 数据利用率如何  3. 是否重要性采样
	目标策略为贪婪或探索可能导致的收敛效果不同，环境不稳定的情况下on-policy算法效果要好
	采样策略感觉都差不多，一个是概率分布的动作，一个是探索性的动作

Q3：数据效率低(和数据利用率不是一个东西)？
A3：prioritized replay buffer

Q4：提高训练稳定性？
A4：experience replay buffer(打破数据相关性)，normalization，target net，优势函数，TRPO限制步长的思想，(SAC?)

Q5：如何设计状态空间？
A5：1. 任务分析，首先要明白agent要实现什么目标，再把这个终极目标拆解成多个合理的小目标
       2. 相关信息筛选，筛选出直接相关信息和间接相关信息。并且对信息做预处理，相当于提取帮神经网络分担了一部分任务。
	比如说吃道具的任务，训练难度：agent与道具的相对距离 < agent与道具的绝对距离 < 游戏图像
       3. 统一性考虑：形式统一和逻辑统一。形式统一是指状态空间每一个位置所表示的特征应该是固定的，多个相同意义的特征每次应该按照相同的顺序放入状态空间中。
	逻辑统一是为了增强RL的泛化能力。比如使用相对距离训练出来的agent要比使用绝对距离训练出来的泛化能力要好，因为换了目标点以后相对距离依旧起作用
       4. 验证状态空间的效果：模仿学习验证(和baseline比较，搭建一个policy，模仿baseline执行action，观察policy性能是否接近baseline的policy)，
	直接验证(直接看reward曲线，一般早期就可以比较win-lost效果)，缺省验证(某一个特征用一个固定值来填入，可以通过曲线确定这个特征的重要性)

Q6：如何设计动作空间？
A6：一般不用设计，环境支持什么操作一般都是设定好的。但是一般需要做action mask，抑制不能执行的动作。后处理的时候要注意屏蔽掉极端的动作(车撞人)。

Q7：如何设计reward？
A7：1. credit assignment：定义好主线奖励的核心地位，然后拆解子目标设计辅助reward，辅助reward的绝对值要小，避免喧宾夺主
       2. 避免某项reward绝对值过大以至于淹没其他reward，避免reward不合理的相对大小导致agent学到异常行为
       2. 常见异常行为-鲁莽：对某个需要杜绝的事件的惩罚给的过小或者忘记给，以至于agent权衡利弊后选择接受惩罚来换取更大的奖励
       3. 常见异常行为-贪婪：如果辅助reward只给奖励不给惩罚，就容易诱导agent学到短视的贪婪策略，陷入支线任务中乐不思蜀，甚至钻空子仅靠支线奖励就获得比主线更多的奖励。
	解决办法：除了给予主线奖励正向reward外，其余辅助reward都设置为惩罚项，这样可以避免agent钻空子，并且可以督促agent快点完成任务。
       4. 常见异常行为-胆怯：如果惩罚项很多并且绝对值相对于主线reward都很大，收到的负反馈完全淹没了主线奖励，agent就容易畏首畏尾，陷入局部最优出不来了。
	解决办法：首先最需要做的是把惩罚调小，突出主线奖励。另外的辅助办法就是减小gamma值，让agent短视，可以一定程度上忽略长期的负收益期望。
       5. reward shaping：根据agent与target的距离定义-α*dist的惩罚
       6. reward设计原则：尽可能稠密(最好每步都有反馈)，能够反映任务目标/子目标逻辑，与状态空间相呼应，控制好各项取值和相对大小，避免异常行为，适时采用reward shaping

Q8：训练相关？
A8：1. 环境可视化：训练一个模型前需要心里先有个底，可视化看看agent采取随机动作的话有没有可能完成任务，明确训练任务的难度
       2. 数据预处理：建议无脑归一化状态空间和reward rescale&clipping，r=clip(r/(std(Return)+ ε),-10,10)，很适合基于episode的算法。
       	采用reward rescale&clipping可以有效降低reward的方差，从而帮助两个网络更加无偏的学习。(没明白，方差小了训练更稳定的意思？)
       	但是只能做rescale不能平移，因为设定reward的符号和相对大小是不可更改的，更改了就会影响回报函数的实际功能。
       3. 超参数设定：
	折扣因子：在能够收敛的前提下尽可能大，根据完成任务需要的步数计算step = 1/(1-γ)，看得远(步数多)实质上是指系统动态演化的跨度大
	模型复杂度：契合状态(CNN,LSTM,MLP)，够用就好
	学习率：1e-4以下，一定要比DeepLearning的1e-3小
       4. 训练指标：
	通过reward_mean/max/min分析训练效果，几乎是唯一最可靠的指标，有时候reward增长到一定趋势又降下来，可以理解为过拟合，只需要保存最好的模型就好了
	通过entropy判断是否有学习，正常学习的模型entropy会不断降低，如果一直维持在初始点就是模型出错了
	通过kl判断前后两次更新策略的变化，正常的kl既不会变化的太多也不会变化的太小，如果变化太小可以考虑收敛的可能
	通过vf_explained_var判断critic学习的情况，正常来讲会逐渐接近1，但是有的情况会先增高后降低(但是并不影响模型学习，我也不懂为啥)
	actor_loss我也不知道咋看，博客上说只要actor_loss曲线和reward_mean形状保持一致，但是形状变化的会晚一些，就是好的
	critic_loss我的理解是不用管他，爱咋变咋变，在一个大的随机环境下几乎是不可能学习到收敛的。观察vf_explained_var就好了


Q8：如何降低学习难度？
A8：连续动作离散化，

Q：MARL环境不稳定？
A：TODO(Liuwenlin) DIMAPG