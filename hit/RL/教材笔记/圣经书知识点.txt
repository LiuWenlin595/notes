贝尔曼方程状态值函数 动作值函数
策略评估 策略改善
值函数迭代方法
同轨策略  离轨策略  为了保证试探，所以使用了同归策略，但是目标策略采取的行动却不一定是最优的，因此引入了离轨策略  
用来更新策略的样本分布是否与策略生成的分布严格相同(采样出的轨迹和更新策略用到的轨迹是否分布相同)
蒙特卡洛方法  P100  P109
重要性采样 加权  平均  增量式实现
分幕式任务  连续型任务
足够多的试探  试探性出发 试探性策略
视界截断 ，处理连续型任务
TD误差  自举   
MC和TD的经典比较  MC是无偏估计，倾向于逼近每个采样回报的最优估计；TD是有偏估计，倾向于学习这个潜在模型的极大似然估计，即估计未来的趋势
★MC和TD方法的不同   方差  偏差  估计趋势  同轨离轨   在线离线  步长  都是采样更新
时序差分方法   P128   P130  
期望Sarsa   离轨策略  方差更小
双Q学习和Double DQN
★one-step和n-step比较：方差  偏差  学习速度   计算量  重要性采样  更新延迟
同轨n-step  离轨n-step算法  n步树回溯
决策时规划  预演算法   蒙特卡洛树搜索
采样更新    期望更新
随机梯度下降和半梯度方法
差分价值函数版本的各种算法
★资格迹的优缺点(n-step的升级版)   加速学习但计算成本高   有效利用数据，适用于数据稀疏或不可重复利用的情况，有效解决MC的在线学习和持续性任务问题
λ可以有效缓解延迟效应引起的方差
前向视图的λ回报  后向试图的λ回报  前向视图和后向视图的特点
在线 λ回报   视界截断
★策略梯度方法的优缺点  先验知识   直接算出动作概率  无需计算价值函数  处理连续动作  动作变化平滑   和MC方法类似的高方差问题  onpolicy方法样本间相关度大
为什么从PG到AC   牺牲偏差减小方差    引入基线的好处(A2C)
REINFORCE算法
重要性采样的缺点   方差大  解决方法  采用平滑因子 或者 clip  以损失偏差的代价减小方差
 