1. RL与有监督学习的不同：延迟收益(试错，评估性反馈)；是否存在EE问题
2. 平稳问题和非平稳问题：动作的真实价值是否随着时间变化
3. UCB：A = argmax(a) [Q(a) + c * sqrt(ln t / N(a))]
4. 收益的相对大小更重要，对每个收益值都加上一个常数C <==> 对每个状态价值加上一个常数C
5. 根据episode可以分三类任务：有限视界，不定视界，无限视界
6. 为什么要试探？ 不试探就没办法遍历所有的(s, a)
7. 如何解决MC方法初始估值不准带来的问题？
如果试探无穷次，这个损失可以忽略，最后一定能收敛；不能试探无穷次的话，可以采用加权MC，降低初始估值的比重
8. off-policy方法：方差大(主要因为IS)，收敛慢
9. 加权重要性采样方差小(每个G的权重都在0~1范围内)，更易收敛，但是有偏；普通重要性采样方差大(每个G的权重理论上无界)，无偏
10. MC只会从幕的尾部开始学习，学习速度很慢，很可能到前面的时候折扣因子gamma^n就把奖励抹除了
11. 5.8/5.9介绍了两种移除无关项来减小IS方差的方法
12. MC在马尔可夫不成立时性能损失较小，因为不需要自举
13. DP/MC/TD的不同：是否自举；采样更新/期望更新；n-step/one-step +  MC专属的问题10、18
14. DQN为什么不需要IS？
Q值更新可以理解为人工指定要更新这个Q，所以不需要考虑当前步，只需要考虑之后的sequence(或者直接上Q值的贝尔曼公式)
15. DQN max贪婪的坏处？   B <- A -> 0，B是一个均值-0.1的正态分布
16. 当处理持续性任务的时候，折扣因子不起作用了，需要借助平均收益和差分版本的TD误差 P250
17. 致命三要素：离轨，自举，函数逼近
18. 整幕更新的坏处：如果一幕中某个差收益的动作被选择的概率比较高，则agent在这一幕中选择这个动作的倾向不会变低，所以很影响训练速度
19. 引入baseline的作用：A的作用，把优势不明显的动作变成劣势动作；减小方差，加速收敛


公式：贝尔曼方程，最优贝尔曼方程，平均和加权的递推公式，MC误差改写为TD误差(P119)，TD(λ) (P288)
代码：策略评估，策略改善，值函数迭代，试探性出发MC，on-policy MC，off-policy MC，Sarsa，Q-learning，n-step Sarsa，n-step Q-learning，
	TD(λ)，REINFORCE，带基线的REINFORCE，AC