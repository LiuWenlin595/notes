1. RL与有监督学习的不同：延迟收益(试错，评估性反馈)；是否存在EE问题
2. 平稳问题和非平稳问题：动作的真实价值是否随着时间变化
3. UCB：A = argmax(a) [Q(a) + c * sqrt(ln t / N(a))]
4. 收益的相对大小更重要，对每个收益值都加上一个常数C <==> 对每个状态价值加上一个常数C
5. 根据episode可以分三类任务：有限视界，不定视界，无限视界
6. 为什么要试探？ 不试探就没办法遍历所有的(s, a)
7. 如何解决MC方法初始估值不准带来的问题？
如果试探无穷次，这个损失可以忽略，最后一定能收敛；不能试探无穷次的话，可以采用加权MC，降低初始估值的比重
8. off-policy方法：方差大(主要因为IS)，收敛慢
9. 加权重要性采样方差小(每个G的权重都在0~1范围内)，更易收敛，但是有偏；普通重要性采样方差大(每个G的权重理论上无界)，无偏
10. MC只会从幕的尾部开始学习，学习速度很慢，很可能到前面的时候折扣因子gamma^n就把奖励抹除了 (前期非贪心的行为比较多的时候尤其如此)
11. 5.8/5.9介绍了两种移除无关项来减小IS方差的方法
12. MC在马尔可夫不成立时性能损失较小，因为不需要自举
13. DP/MC/TD的不同：是否自举；采样更新/期望更新；n-step/one-step +  MC专属的问题10、18
14. DQN为什么不需要IS？
Q值更新可以理解为人工指定要更新这个Q，所以不需要考虑当前步，只需要考虑之后的sequence(或者直接上Q值的贝尔曼公式)
15. DQN max贪婪的坏处？   B <- A -> 0，B是一个均值-0.1的正态分布
16. 当处理持续性任务的时候，折扣因子不起作用了，需要借助平均收益和差分版本的TD误差 P250
17. 致命三要素：离轨，自举，函数逼近，函数逼近是必须的，自举有利于学习速度，离轨条件没有那么严格，可以用其他方法代替探索
18. 整幕更新的坏处：如果一幕中某个差收益的动作被选择的概率比较高，则agent在这一幕中选择这个动作的倾向不会变低，所以很影响训练速度。在线方法比如TD或TDλ就避免了这个问题
19. 引入baseline的作用：A的作用，把优势不明显的动作变成劣势动作；减小方差，加速收敛
20. agent不能改变的东西都可以理解为外部的，即环境的一部分(这种思想特别好用)
21. Dyna-Q：一次数据执行一次，更新一次，之前的数据规划n次
22. 采样更新要比期望更新好：一是因为不确定环境下没办法期望更新，二是因为相同时间内期望更新只能更新一个Q，而采样更新可以更新很多不同的Q
23. critic和actor的函数逼近参数更新公式都是通过最优化来的，一个是梯度下降损失函数，一个是梯度上升性能指标
24. 函数逼近所有值函数共用一套参数，而表格型是自己更新自己的，所以函数逼近不满足策略改进定理，
	改变一个状态的值函数会影响其他状态的值函数，所以改变后产生的策略的期望收益不一定高
25. 对于有限数据，TDλ作用很大，因为可以更有效地利用数据；对于离线数据，TDλ作用没有那么大，因为数据有很多，所以应该把注意力放在如何去学习更多的数据
26. 带资格迹的AC算法，actor和critic都可以分别用资格迹，还要留意一下参数更新的TD误差虽然写法一样，但是代表的含义不一样

式：贝尔曼方程，最优贝尔曼方程，平均和加权的递推公式，MC误差改写为TD误差(P119)，TD(λ) (P288)
代码：策略评估，策略改善，值函数迭代，试探性出发MC，on-policy MC，off-policy MC，Sarsa，Q-learning，n-step Sarsa，n-step Q-learning，
	TD(λ)，REINFORCE，带基线的REINFORCE，AC