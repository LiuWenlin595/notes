1. normalization的作用：1.加速收敛  2.使得模型对学习率不敏感
2. MIT研究员在norm网络后加入了高斯噪声打乱了原来的独立同分布发现，效果依然不受影响。猜测normalization的作者理解的不对，又对loss求一阶导和二阶导发现具有良好的性质(不存在震荡波动)，
说明norm之所以起作用是因为直接作用在了loss函数，使得loss函数变成了一个一阶、二阶均平滑的函数
3. Relu神经元坏死的解释：某一次传给激活函数的输入单元wx+b非常大(虽然不明白为什么会有这种输入)，就会导致loss很大，进而更新的梯度很大，进而b = b - α*grad导致b变成一个很大的负值，
	至此以后因为b负的很多，所以后续正常的wx+b都会<0，梯度为0，这个神经元就坏死了，解决方法可以用LeakyRelu或者ELU代替
4. 验证集loss上升且acc上升： 模型过于自信，少量错误样本的预测概率值过大导致带来了巨大的loss
问题原因是过拟合，即训练集与验证集样本分布不一致 或者 训练集样本太少。 实际不用太关注loss，只要acc上去了就行
对应的，强化学习也是一样，reward上去了就行，不用太关注loss
5. 正则化：对某一问题加以先验的限制或约束以达到某种特定目的的一种手段或操作，主要用于防止过拟合
6. 归一化(min-max normalization)：找到某种映射关系，将原数据映射到[a,b]，提升模型精度：归一化后，不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性
7. 标准化(Z-score normalization)：用大数定理将数据转化为一个标准正态分布，加速模型收敛：标准化后，最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解
8. 归一化和标准化的区别：归一化每个特征值的缩放只跟对应特征的最大值最小值有关，标准化每个特征值的缩放需要考虑batchsize个其他样本对应特征的值，对这些值做正态分布
9. 常用的归一化：batch norm (BN)，layer norm (LN)，Instance norm (IN)， Group norm (GN)，后面两个图像中用的比较多
10. 归一化是映射到[0 1]还是[-1 1]：大部分网络偏好[-1 1]输入，但实际在网络训练了以后差别不大，因为权重会自动调整。(没有测试过)
	归一化的目的是方便梯度的传播，防止梯度爆炸或梯度消失。如果因为激活函数是Relu而归一化到[0 1]是不正确的，因为Relu就是需要有正数负数才能体现非线性的作用
11. Gram矩阵：xi*xj 、黑森矩阵：f对xi xj的二阶导
12. CPU利用率：当程序涉及到大量的计算，CPU利用率就高； CPU负载：一段时间内正在使用和等待使用CPU的平均任务数
	每个内核的负载<0.7比较好，所以CPU负载应该<=0.7 * CPU数 * 每个CPU内核数
13. apt install htop：很好用的一个工具，可以查看CPU memory使用情况
14. 静态图(tf1)和动态图(torch)的区别：动态图是运算和搭建同时进行，灵活，容易debug；静态图先搭建好图然后进行运算，速度快，不灵活
15. BN的提出是为了解决ICS问题：随着网络深度的增加，高层的网络参数要不断适应新的数据分布，会使得训练周期变长；高层的网络输出求导存在梯度消失问题  (BN可以把数据归一化)